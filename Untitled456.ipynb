{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562fe097-ba7e-4591-abc8-ee4d15e737ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Dataset Shape: (36, 6)\n",
      "Columns: ['States/UTs', 'Geographical_-_Area_(sq_km)', 'RFA_-_(sq_km)', 'Volume_of_Growing_Stock_(m._cum)_-_In_Forest', 'Volume_of_Growing_Stock_(m._cum)_-_In_TOF', 'Volume_of_Growing_Stock_(m._cum)_-_Total']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_77004\\2263114148.py:45: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"ffill\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.9178 - accuracy: 0.0833\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8552 - accuracy: 0.0833\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8003 - accuracy: 0.1944\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.7507 - accuracy: 0.2778\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7118 - accuracy: 0.3611\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6767 - accuracy: 0.6111\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.8056\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6220 - accuracy: 0.9167\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.9167\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.9167\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 821us/step - loss: 0.5693 - accuracy: 0.9167\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.9167\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.9167\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.9167\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 753us/step - loss: 0.4899 - accuracy: 0.9167\n",
      "✅ Carbon Credit Fraud Detection Pipeline Completed Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Temp\\ipykernel_77004\\2263114148.py:133: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(nn_model, os.path.join(MODEL_DIR, \"carbon_fraud_model.h5\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Carbon Credit Fraud Detection\"\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"Table_5.5_Carbon_Stock_in_India_Forests.csv\")\n",
    "\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model\")\n",
    "META_DIR = os.path.join(BASE_DIR, \"metadata\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(META_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.strip().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# ============================================================\n",
    "# BASIC CLEANING\n",
    "# ============================================================\n",
    "\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a pseudo issuer / project id for graph modeling\n",
    "df[\"issuer_id\"] = df.index % 10\n",
    "df[\"project_id\"] = df.index\n",
    "\n",
    "# ============================================================\n",
    "# GRAPH CONSTRUCTION\n",
    "# ============================================================\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    G.add_node(row[\"project_id\"], carbon_stock=row[df.columns[1]])\n",
    "    G.add_edge(row[\"issuer_id\"], row[\"project_id\"])\n",
    "\n",
    "# Graph-based features\n",
    "df[\"degree_centrality\"] = df[\"project_id\"].apply(\n",
    "    lambda x: nx.degree(G, x)\n",
    ")\n",
    "\n",
    "df[\"issuer_connection_count\"] = df[\"issuer_id\"].apply(\n",
    "    lambda x: nx.degree(G, x)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "X = df[numeric_cols]\n",
    "\n",
    "# ============================================================\n",
    "# SCALING\n",
    "# ============================================================\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# ============================================================\n",
    "# ANOMALY DETECTION MODEL\n",
    "# ============================================================\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.08,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fraud_labels = iso_forest.fit_predict(X_scaled)\n",
    "df[\"fraud_label\"] = fraud_labels\n",
    "df[\"fraud_flag\"] = df[\"fraud_label\"].apply(lambda x: \"Fraud\" if x == -1 else \"Legit\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE MODEL (.pkl)\n",
    "# ============================================================\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"carbon_fraud_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(iso_forest, f)\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL NEURAL MODEL (.h5)\n",
    "# ============================================================\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(X_scaled.shape[1],)),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "nn_model.fit(\n",
    "    X_scaled,\n",
    "    (fraud_labels == -1).astype(int),\n",
    "    epochs=15,\n",
    "    batch_size=8,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "save_model(nn_model, os.path.join(MODEL_DIR, \"carbon_fraud_model.h5\"))\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS (.json)\n",
    "# ============================================================\n",
    "\n",
    "results_json = df[[\n",
    "    \"project_id\",\n",
    "    \"issuer_id\",\n",
    "    \"fraud_flag\"\n",
    "]].to_dict(orient=\"records\")\n",
    "\n",
    "with open(os.path.join(RESULT_DIR, \"fraud_predictions.json\"), \"w\") as f:\n",
    "    json.dump(results_json, f, indent=4)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE METADATA (.yaml)\n",
    "# ============================================================\n",
    "\n",
    "schema = {\n",
    "    \"project\": \"AI-Driven Carbon Credit Fraud Detection\",\n",
    "    \"domain\": \"Web3 + ClimateTech\",\n",
    "    \"ml_technique\": [\n",
    "        \"Graph ML\",\n",
    "        \"Isolation Forest\",\n",
    "        \"Neural Anomaly Detection\"\n",
    "    ],\n",
    "    \"features_used\": numeric_cols,\n",
    "    \"labels\": {\n",
    "        \"Fraud\": -1,\n",
    "        \"Legit\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(META_DIR, \"schema.yaml\"), \"w\") as f:\n",
    "    yaml.dump(schema, f)\n",
    "\n",
    "# ============================================================\n",
    "# README\n",
    "# ============================================================\n",
    "\n",
    "readme_text = \"\"\"\n",
    "AI-Driven Carbon Credit Fraud Detection\n",
    "\n",
    "Problem:\n",
    "Detect fake or duplicate carbon credits in blockchain-based carbon markets.\n",
    "\n",
    "ML Stack:\n",
    "- Graph-based Feature Engineering\n",
    "- Isolation Forest (Anomaly Detection)\n",
    "- Neural Network Validation\n",
    "\n",
    "Outputs:\n",
    "- Trained Models (.pkl, .h5)\n",
    "- Fraud Predictions (.json)\n",
    "- Metadata (.yaml)\n",
    "\n",
    "Use-cases:\n",
    "- Carbon Exchanges\n",
    "- ESG Audits\n",
    "- Climate Finance Risk Assessment\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"README.txt\"), \"w\") as f:\n",
    "    f.write(readme_text)\n",
    "\n",
    "print(\"✅ Carbon Credit Fraud Detection Pipeline Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bc5ab-68be-43c0-b8bf-07bdd2c36c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
